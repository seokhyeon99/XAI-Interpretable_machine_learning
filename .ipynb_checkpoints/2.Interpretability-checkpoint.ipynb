{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "various definition of interpretability:  \n",
    "  1. the degree to which a human can understand the cause of a decision.  \n",
    "  2. the degree to which a human can consistently predict the model's result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Importance of Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In predictive modelling, we have to make a trade-off:\n",
    "  - Do you simply want to know what is predicted, or why the prediction was made and possibly paying for the interpretability with a drop in accuracy? (It depends on what the situation is.)  \n",
    "  \n",
    "People usually have a curiousness when they go thorough unexpected events. Closely related to learning is the human desire to find meaning in the world.  \n",
    "- Machine learning model itself becomes a source of knowledge, instead of the data. Interpretability allows to tap into this additional knowledge captured by the model.  \n",
    "- By default most machine learning models pick up biases from the training data. Interpretability can be a iuseful debugging tool to detect the biases in the models.  \n",
    "- The process of integrating machines and algorithms into our daily lives demands interpretability to increase social acceptance.  \n",
    "\n",
    "Traits for the machine learning model to be able to explain decisions(Doshi-Velez and Kim 2017):  \n",
    "- Fairness: Making sure the predictions are unbiased and not discrminating against protected.  \n",
    "- Privacy: Ensuring that sensitive information in the data is protected.  \n",
    "- Reliability or Robustness: Test that small changes in the input don't lead to big changes in the prediction.  \n",
    "- Causality: Check if only causal relationships are picked up.  \n",
    "- Trust: It is easier for humans to trust a system that explains its decisions compared to a black box.  \n",
    "  \n",
    "When we do not need interpretability:  \n",
    "- if the model has no significant impact\n",
    "- when the problem is well-studied\n",
    "- if there is a mismatch in the objectives of the creator and the user of a model, Interpretability might cause problems with users fooling a system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Taxonomy of Interpretability Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Intrinsic or post hoc?\n",
    "   * Intrinsic interpretability means selecting and training a machine learning model that is considered to be intrinsically interpretable(e.g. short decision trees)\n",
    "   * Post hoc interpretability means selecting and training a black box model(e.g. neural network) and applying interpretability methods after the training(i.e. measuring the feature importance)\n",
    "* The interpretability method according to their outcomes:\n",
    "   * Feature summary statistic\n",
    "   * Feature summary visualization: some feature summaries can only be visualized and not meaningfully be placed in a table(e.g. partial dependence of a feature)\n",
    "   * Model internals(e.g. learned weights): The interpretation of intrinsically interpretable models falls under this category.\n",
    "   * Data point: This category includes all methods that return data paoints (can be existing or newly created) to make a model interpretable. Interpretability methods that ouput new data points work well for images and text but is less useful for tubular data with hundreds of features.\n",
    "   * Intrinsically interpretable model: the interpretable model themselves are interpreted by internal model parameter or feature summary statistics.\n",
    "* Model-specific or model-agnostic?\n",
    "   * Model-specific interpretation tools are limited to specific model classes. the interpretation of intrinsically interpretable models is always model-specific.\n",
    "   * Model-agnostic tools can be used on any machine learning model and are usually post hoc. These agnostic methods usually operate by analysing feature input and output pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Scope of Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithm transparency\n",
    "   * Algorithm transparency is about how the algorithm learns a model from the data and what kind of relationships it is capable of picking up.\n",
    "   * Algorithms for linear models are well studied and understood, so they score high in transparency. On the other hand, if how some methods exactly work is not clear, they are less transparent.\n",
    "* Global, Holistic Model Interpretability\n",
    "   * This level of interpretability is about understanding how the model makes the decisions, based on a holistic view of its features and each of the learned components like weights, parameters, and structures.\n",
    "   * global model interpretability is very hard to achieve in practice. \n",
    "* Global Model Interpretability on a Modular Level\n",
    "   * While global model interpretability is usually out of reach, there is a better chance to understand at least some models on a modular level.\n",
    "   * this interpretability can not be work on every model. For example, in the case of interpretation of a single weight, the weight always come with other inputs.\n",
    "* Local Interpretability for a Sigle Prediction\n",
    "   * You can zoom in on a single instance and examine what kind of prediction the model makes for this input, and why it made this decision.\n",
    "   * Local expectation can be more accurate compared to global explanation.\n",
    "* Local Interpretability for a Group of Prediction\n",
    "   * The global methods can be applied by taking the group of instances, pretending it's the complete datasets, and using the global methods on this subset.\n",
    "   * The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Evaluating Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no real consensus on what interpretability in machine learning is. Also it is not clear how to measure it.  \n",
    "\n",
    "* Approaches for Evaluating the Interpretability Quality(Doshi-Velez and Kim proposed three major levels(2017))  \n",
    "     * Application level evaluation (real task): Put the explanation into the product and let the end user test it. It is conducted with the domain experts.\n",
    "     * Human level evaluation (simple task): simplified application level evaluation. it is conducted with lay humans.\n",
    "     * Function level evaluation (proxy task): This works best when the class of models used was already evaluated by someone else in a human level evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Properties of Explanations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Properties of Explanation Methods  \n",
    "   * Expressive Power:  \n",
    "           structure of the explanation. i.e. If-Then rules, decision tree, or something else.\n",
    "   * Translucency:  \n",
    "           how much the explanation method relies on machine learning model. While methods relying on intrinsically interpretable models are highly translucent, methods only relying on manipulating inputs and observing the predictions have zero translucency.\n",
    "           The advantage of high translucency is that the method can rely on more information to generate explanations. The advantage of low translucency is that the explanation method is more portable.\n",
    "   * Portability:  \n",
    "           the range of machine learning models with which the explanation method can be used. Methods with a low translucency have a higer portability. Surrogate models might has the highest portability.\n",
    "   * Algorithmic Complexity:\n",
    "           computational complexity of the methods  \n",
    "        \n",
    "- Properties of Individiual Explanations  \n",
    "   * Accuracy:\n",
    "           How well an explanation predicts data\n",
    "   * Fidelity:\n",
    "           How well the explanation approximates the prediction of the black box model. High fidelity is one of the most important properties of an explanation. Accuracy and fidelity are closely related. If the black box model has high accuracy and the explanation has high fidelity, the explanation also has high accuracy.\n",
    "   * Consistency:\n",
    "           How similar the explanations are between models that have been trained on the same task and that produce similar predictions. \n",
    "   * Stability:\n",
    "           How siilar the explanations are for similar instances.\n",
    "   * Comprehensibility:\n",
    "           How well humans understand the explanations\n",
    "   * Certainty:\n",
    "           How well the explanation reflect the certainity of the machine learning model.\n",
    "   * Degree of Importance:\n",
    "           How well the explanation reflect the importance of features or parts of the explanation.\n",
    "   * Novelty:\n",
    "           How well the explanation reflect whether a data instance to be explained comes from a \"new\" region far removed from the distribution of training data. The higher the novelty is, the more likley it is that the model will have low certainty due to lack of data.\n",
    "   * Representativeness:\n",
    "           How many instances an explanation covers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Human-friendly Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an explanation for an event, humans prefer short explanations (just 1 or 2 causes), which contrast the current situation with a situation where the event would not have happened. Especially abnormal causes make good explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 What is an explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Explanations are social interactions between the explainer and the explainee (receiver of the explanation) and therefore the social context has a huge influence on the actual content of the explanation.\n",
    "    - An explanation is the answer to a why-question (Miller 2017).\n",
    "    - The term “explanation” means the social and cognitive process of explaining, but it’s also the product of these processes. The explainer can be a human or a machin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 What is a \"good\" explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explanations are contrastive (Lipton 2016)\n",
    "        Humans don’t want a complete explanation for a prediction but rather compare what the difference were to another instance’s prediction (could also be an artificial one).\n",
    "* Explanations are selected\n",
    "        Make the explanation very short, give only 1 to 3 reasons, even if the world is more complex.\n",
    "* Explanations are social\n",
    "        Be mindful of the social setting of your machine learning application and of the target audience.\n",
    "* Explanations focus on the abnormal\n",
    "        If one of the input features for a prediction was abnormal in any sense (like a rare category of a categorical feature) and the feature influenced the prediction, it should be included in an explanation, even if other ‘normal’ features have the same influence on the prediction as the abnormal one.\n",
    "* Explanations are truthful\n",
    "        The explanation should predict the event as truthfully as possible, which is sometimes called fidelity in the context of machine learning.\n",
    "* Good explanations are coherent with prior beliefs of the explainee.\n",
    "        Good explanations are consistent with prior beliefs. This one is hard to infuse into machine learning and would probably drastically compromise predictive accuracy.\n",
    "* Good explanations are general and probable.\n",
    "        Generality is easily measured by a feature’s support, which is the number of instances for which the explanation applies over the total number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Reference: https://christophm.github.io/interpretable-ml-book/interpretability.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
